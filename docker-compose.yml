services:
  # -----------------------------
  # ZOOKEEPER
  # -----------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  # -----------------------------
  # KAFKA
  # -----------------------------
  kafka:
    image: confluentinc/cp-kafka:6.2.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    volumes:
      - kafka-data:/var/lib/kafka/data

      # -----------------------------
  # KAFKA CONSUMER (Python)
  # -----------------------------
  kafka-consumer:
    image: python:3.11-slim
    container_name: kafka-consumer
    depends_on:
      - kafka
      - namenode
      - datanode
    volumes:
      - ./kafka_to_hdfs_copy.py:/app/kafka_to_hdfs_copy.py
    working_dir: /app
    command: >
      bash -c "
      pip install kafka-python hdfs pyarrow &&
      python kafka_to_hdfs_copy.py
      "




  # -----------------------------
  # HDFS NAMENODE
  # -----------------------------
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - "9870:9870"   # Web UI
      - "8020:8020"   # RPC port
      - "7000:7000"   # JMX exporter
    volumes:
      - ./monitoring/jmx:/opt/jmx:ro
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
      HADOOP_NAMENODE_OPTS: "-javaagent:/opt/jmx/jmx_prometheus_javaagent-0.20.0.jar=7000:/opt/jmx/hadoop.yml"
    env_file:
      - ./config
  # -----------------------------
  # HDFS DATANODE
  # -----------------------------
  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    hostname: datanode
    command: ["hdfs", "datanode"]
    ports:
      - "9864:9864"   # Web UI
      - "7001:7001"   # JMX exporter
    volumes:
      - ./monitoring/jmx:/opt/jmx:ro
    environment:
      HADOOP_DATANODE_OPTS: "-javaagent:/opt/jmx/jmx_prometheus_javaagent-0.20.0.jar=7001:/opt/jmx/hadoop.yml"
    env_file:
      - ./config

  # -----------------------------
  # SPARK
  # -----------------------------
  spark-master:
    image: spark:latest
    hostname: spark-master
    container_name: spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master",
              "--host", "spark-master", "--port", "7077", "--webui-port", "8080"]
    ports: ["7077:7077", "8080:8080"]
    volumes:
      - ./spark:/opt/spark/apps
    depends_on: [namenode]
    

  spark-worker-1:
    image: spark:latest
    hostname: spark-worker-1
    container_name: spark-worker-1
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker",
              "spark://spark-master:7077"]
    volumes:
      - ./spark:/opt/spark/apps
    depends_on: [spark-master]

volumes:
  kafka-data:
  namenode-data:
  datanode-data:
  