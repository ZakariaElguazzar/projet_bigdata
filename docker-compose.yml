services:
  # -----------------------------
  # ZOOKEEPER
  # -----------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  # -----------------------------
  # KAFKA
  # -----------------------------
  kafka:
    image: confluentinc/cp-kafka:6.2.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data

  #-----------------------------
  # KAFKA PRODUCER (Python)
  #-----------------------------
  kafka-producer:
    build: ./producer
    container_name: kafka-producer
    depends_on:
      - kafka
    restart: unless-stopped

  # -----------------------------
  # KAFKA CONSUMER (Python)
  # -----------------------------
  kafka-consumer:
    build: ./consumer
    container_name: kafka-consumer
    depends_on:
      - kafka
      - namenode
      - datanode
      - kafka-producer
    restart: unless-stopped
    

  # -----------------------------
  # HDFS NAMENODE
  # -----------------------------
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    command: >
      bash -c "
      hdfs namenode &&
      /opt/hadoop-init/hdfs-init.sh
      "
    ports:
      - "9870:9870"
      - "8020:8020"
      - "7000:7000"
    volumes:
      - ./monitoring/jmx:/opt/jmx:ro
      - ./hdfs-init.sh:/opt/hadoop-init/hdfs-init.sh
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
      HADOOP_NAMENODE_OPTS: "-javaagent:/opt/jmx/jmx_prometheus_javaagent-0.20.0.jar=7000:/opt/jmx/hadoop.yml"
    env_file:
      - ./config


  # -----------------------------
  # HDFS DATANODE
  # -----------------------------
  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    hostname: datanode
    command: ["hdfs", "datanode"]
    ports:
      - "9864:9864"   # Web UI
      - "7001:7001"   # JMX exporter
    volumes:
      - ./monitoring/jmx:/opt/jmx:ro
    environment:
      HADOOP_DATANODE_OPTS: "-javaagent:/opt/jmx/jmx_prometheus_javaagent-0.20.0.jar=7001:/opt/jmx/hadoop.yml"
    env_file:
      - ./config

  # -----------------------------
  # SPARK MASTER
  # -----------------------------
  spark-master:
    image: spark:latest
    hostname: spark-master
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./spark:/opt/spark/apps
      - ./config:/opt/hadoop/etc/hadoop
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    depends_on:
      - namenode
    command: >
      bash -c "
      chmod +x /opt/spark/apps/start-spark-jobs.sh &&
      /opt/spark/apps/start-spark-jobs.sh
      "
    restart: unless-stopped


  # -----------------------------
  # SPARK WORKER
  # -----------------------------
  spark-worker-1:
    image: spark:latest
    hostname: spark-worker-1
    container_name: spark-worker-1
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker",
              "spark://spark-master:7077"]
    volumes:
      - ./spark:/opt/spark/apps
      - ./config:/opt/hadoop/etc/hadoop
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    depends_on:
      - spark-master
    restart: unless-stopped

  # -----------------------------
  # HDFS EXPORTER (BRIDGE)
  # -----------------------------
  hdfs-exporter:
    build: ./exporter
    container_name: hdfs-exporter
    ports:
      - "8000:8000"
    depends_on:
      - namenode
    restart: unless-stopped

  # -----------------------------
  # PROMETHEUS
  # -----------------------------
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    depends_on:
      - namenode
      - datanode

  # -----------------------------
  # GRAFANA
  # -----------------------------
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin

  # -----------------------------
  # APACHE AIRFLOW
  # -----------------------------

  airflow-db:
    image: postgres:13
    container_name: airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - airflow-db-data:/var/lib/postgresql/data

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow-webserver
    depends_on:
      - airflow-db
      - spark-master
      - kafka
      - namenode

    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__WEBSERVER__RBAC: "True"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    ports:
      - "8085:8080"
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username admin --firstname Air --lastname Flow --role Admin --email admin@example.com --password admin &&
      airflow webserver
      "

  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: scheduler
    restart: unless-stopped


volumes:
  kafka-data:
  namenode-data:
  datanode-data:
  airflow-db-data:
